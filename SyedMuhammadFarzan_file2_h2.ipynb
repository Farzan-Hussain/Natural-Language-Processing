{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Pipeline on Larger Data"
      ],
      "metadata": {
        "id": "bE-whAL7GsyU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLJ5QRiHvoc9",
        "outputId": "53c29639-2094-4a39-deec-29a16f818f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Connecting to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data\n",
        "file_path = \"/content/drive/MyDrive/Datasets/spam.csv\"\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Working around the encoding error while loading the dataset\n",
        "encodings = [\"utf-8\", \"latin1\", \"iso-8859-1\", \"utf-16\"]\n",
        "\n",
        "for encoding in encodings:\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, encoding=encoding)\n",
        "        print(\"File read successfully using encoding:\", encoding)\n",
        "        break  # Exit the loop if the file is read successfully\n",
        "    except UnicodeDecodeError:\n",
        "        print(\"Failed to read with encoding:\", encoding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WXF9VN7vx3v",
        "outputId": "278319bd-30bc-46db-825c-16f07cc0a8fd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to read with encoding: utf-8\n",
            "File read successfully using encoding: latin1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming the columns\n",
        "data = data.rename(columns={'v1': 'label', 'v2': 'message'})\n",
        "\n",
        "# Replace with binary\n",
        "data['label'] = data['label'].replace({'ham': 1, 'spam': 0})\n",
        "\n",
        "# Dropping unnecessary columns\n",
        "data.drop(columns = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace = True)\n",
        "\n",
        "# Viewing the data\n",
        "data.head"
      ],
      "metadata": {
        "id": "Kb-TiMuTwzpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a34fd593-888c-4b02-8f95-58d8896ed877"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of       label                                            message\n",
              "0         1  Go until jurong point, crazy.. Available only ...\n",
              "1         1                      Ok lar... Joking wif u oni...\n",
              "2         0  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3         1  U dun say so early hor... U c already then say...\n",
              "4         1  Nah I don't think he goes to usf, he lives aro...\n",
              "...     ...                                                ...\n",
              "5567      0  This is the 2nd time we have tried 2 contact u...\n",
              "5568      1              Will ÃŒ_ b going to esplanade fr home?\n",
              "5569      1  Pity, * was in mood for that. So...any other s...\n",
              "5570      1  The guy did some bitching but I acted like i'd...\n",
              "5571      1                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing libraries\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "iUEkiCuqyDgA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.6, random_state=42, stratify=data['label'])"
      ],
      "metadata": {
        "id": "OqYbqtsHw3Gz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the best performing pipeline\n",
        "pipeline_1 = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', xgb.XGBClassifier())\n",
        "])\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'tfidf__max_features': [1000, 2000, 3000],\n",
        "    'clf__learning_rate': [0.01, 0.1, 0.2],\n",
        "    'clf__max_depth': [3, 5, 7],\n",
        "    'clf__n_estimators': [100, 200, 300]\n",
        "}\n",
        "\n",
        "# Create randomized search\n",
        "random_search_1 = RandomizedSearchCV(estimator=pipeline_1, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)"
      ],
      "metadata": {
        "id": "mIFNUZ71yrMz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the randomized search to find the best model\n",
        "random_search_1.fit(train_data['message'], train_data['label'])\n",
        "\n",
        "# Get the best model from the random search\n",
        "best_model_1 = random_search_1.best_estimator_\n",
        "\n",
        "# Make predictions using the best model\n",
        "predictions_1 = best_model_1.predict(test_data['message'])\n",
        "\n",
        "# Now you can evaluate the predictions using the appropriate evaluation metrics\n",
        "precision_1 = precision_score(test_data['label'], predictions_1, pos_label=0)\n",
        "recall_1 = recall_score(test_data['label'], predictions_1, pos_label=0)\n",
        "f1_1 = f1_score(test_data['label'], predictions_1, pos_label=0)\n",
        "\n",
        "# Create a classification report\n",
        "classification_report_1 = classification_report(test_data['label'], predictions_1)"
      ],
      "metadata": {
        "id": "JDFzeSGEy1Tp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print evaluation metrics\n",
        "print(\"Pipeline 1:\")\n",
        "print(f\"Precision: {precision_1}\")\n",
        "print(f\"Recall: {recall_1}\")\n",
        "print(f\"F1 Score: {f1_1}\")\n",
        "print(classification_report_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaTydonkD1WJ",
        "outputId": "99fe3d8c-bb21-4f68-f963-e98c7b83b848"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline 1:\n",
            "Precision: 0.9413333333333334\n",
            "Recall: 0.7879464285714286\n",
            "F1 Score: 0.8578371810449574\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.79      0.86       448\n",
            "           1       0.97      0.99      0.98      2896\n",
            "\n",
            "    accuracy                           0.97      3344\n",
            "   macro avg       0.95      0.89      0.92      3344\n",
            "weighted avg       0.96      0.97      0.96      3344\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-coC67Q4zC6u"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}